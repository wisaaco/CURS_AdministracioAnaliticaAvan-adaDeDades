{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unitat 9. Introducció a data lakes i data warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquests dos conceptes fan referència a arquitectures on s'emmagatzemen dades amb l'objectiu de facilitar la integració de dades de diferents serveis d'una organització a llarg plaç. Aquest tipus d'arquitectures impliquem que al darrera hi ha un projecte de governanza de les dades. La presència d'aquestes entitats s'acompanya d'una guia de bones pràctiques per a la recopilació, integració i organització de dades que ha de ser entesa per els departaments d'una organització.<br/>\n",
    "\n",
    "Naturalment, està format per clusters d'equip de diferent naturaleza i composició, a més inclou un grup heterogeni de programari que centralitcen les dades des d'una perspectiva d'explotació oferim punts d'entrada per als usuaris on puguin consultar  dades de intereses.\n",
    "\n",
    "En aquestes arquitectures hi ha processos operacionals com còpies de seguretat, xifrat i encriptació, autenticació i accés, eines per a l'execució distribuïda de dades, etc. \n",
    "\n",
    "Atès que són conceptes propis de l'enginyeria i perfils molt especialitzats com el d'arquitecte de cloud o de dades, **en aquest capítol ens centrarem  a adquirir nocions bàsiques de tots dos conceptes i entendre els avantatges que aquests sistemes aporten a una organització i veurem con funciona des de el punt de vista d'un usuari**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Nocions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quan una organització es subdivideix en departaments, grups i projectes, cadascun genera i depèn d’uns dades específiques. Una visió holística de les dades sol ser complicada si no hi ha una clara governança de les dades. En aquesta es defineix què registrar, com registrar-ho, durant quant de temps, i una de les qüestions importants és on, entre altres factors.\n",
    "\n",
    "##### Aquest \"on\" és un *data lake* i/o un *warehouse*.\n",
    "\n",
    "#### La metàfora\n",
    "Un llac (*lake*) o un dipòsit (*warehouse*) no són més que metàfores relacionades amb el cicle de l’aigua i el flux de dades. Les dades són com l’aigua; moltes circulen dins l’organització sense que sigui necessari guardar-les. No obstant això, en certes situacions és convenient registrar i mantenir algunes, i per això es forma un llac on queda aquesta acumulació de dades/aigua.\n",
    "\n",
    "- En un llac (*lake*) l’aigua s’emmagatzema en cru, és a dir, no es fan processos de neteja o tractament.\n",
    "\n",
    "- En un dipòsit o tanc (*warehouse*), l’aigua és tractada, és a dir, es realitzen processos ETL (extracció, transformació, càrrega).\n",
    "\n",
    "Segons el context, ambdós termes poden acabar confonent-se. Avui dia, certes eines pensades per donar suport a funcions *lake* ofereixen mecanismes per fer transformacions.\n",
    "\n",
    "\n",
    "![ETL](datalakes/ETL.jpg \"ETL\")\n",
    "\n",
    "A nivell pràctic, els grans proveïdors de *Cloud* com Google, AWS (Amazon), Azure (Microsoft), Alibaba, entre d’altres, ofereixen serveis específics que faciliten la integració i una perspectiva centralitzada de les dades. Aquesta centralització es refereix al coneixement de la seva existència, els seus metadades, i el seu oferiment a altres usuaris dins de l’organització.\n",
    "\n",
    "A la figura veiem el panell de creació de *Lakes* dins de Google Cloud (aquest servei dins del seu entorn es diu `Dataplex`), on es poden apreciar funcions d’administració, seguretat, processos operacionals, gestió d’atributs, perfils de verificació de les dades i avaluació de les mateixes, entre altres.\n",
    "![Google Lake](datalakes/googlelake.png  \"Google Lake\")\n",
    "\n",
    "A AWS també trobem el mateix servei, com `AWS Lake Foundation`, on s'integren processos ETL.  \n",
    "![AWS Lake](datalakes/AWS.webp  \"AWS Lake\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existeixen **alternatives obertes i gratuïtes** per crear llacs de dades. La més coneguda és [Apache Hudi](https://hudi.apache.org/), dins de l'ecosistema d'[Apache](https://www.apache.org/). Aquesta fundació acull la major part de projectes de codi obert relacionats amb la web, incloent-hi les principals bases de dades i models de computació, com Apache Spark (similar al que implementa Google), que ofereix analítica sobre grans volums de dades.\n",
    "\n",
    "![Apache Hudi](datalakes/ApacheHudi.png  \"Apache Hudi\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esperam que, després d’aquests breus comentaris i exemples, s'entengui que la configuració i la posada en marxa d'un llac de dades requereix perfils especialitzats en Administració de Sistemes, Arquitectura Cloud o Arquitectura de Dades.\n",
    "\n",
    "A més, estan dissenyats per al processament de grans quantitats de dades en molt poc temps. Per això, es necessiten llibreries especialitzades en computació paral·lela o distribuïda. És a dir, no és habitual utilitzar Jupyter Notebooks o eines similars com Google Colab per executar scripts de Pandas en un llac de dades. En aquests casos, hem de recórrer a llibreries com Spark i paradigmes de programació de \"divideix i venceràs\" (MapReduce).\n",
    "\n",
    "Per fer-nos una idea, imaginau que hem de traduir tota una biblioteca de documents. Una sola persona trigarà molt de temps, però si tenim 100 persones, trigarem gairebé 100 vegades menys (tot i que no és proporcional, ja que cal considerar el temps que implica organitzar aquestes persones i repartir la feina). En aquest sistema, cada persona podria, al seu torn, distribuir tasques de traducció de cada capítol d'un llibre. Imaginau com de ràpid seria si som capaços de dividir el problema d'aquesta manera. Això sí, necessitam una infraestructura capaç de repartir les feines entre treballadors, coordinar-los i gestionar els resultats de tots ells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aportacions: què aporta un data lake?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Què us volem transmetre i ensenyar amb aquesta unitat?\n",
    "\n",
    "Tingueu en compte que, a més de la infraestructura, darrere d'un llac de dades hi ha una evolució en l'organització dels mateixos. Una política de transparència i coherència on tots els perfils d'una organització se’n beneficien.\n",
    "\n",
    "Us plantegem dos escenaris! Segur que us hi heu trobat alguna vegada...\n",
    "\n",
    "Nota: Font de les imatges: Referències [5] *The Analytics Setup Guidebook: How to build scalable analytics & BI stacks in the modern cloud era.* Huy Nguyen, Ha Pham, Cedric Chin. Holistics, 2020.\n",
    "\n",
    "#### Primer escenari\n",
    "\n",
    "Un usuari que vol conèixer un indicador (KPI) de l'organització (per exemple, vendes totals, sol·licitants d'una ajuda, projectes subvencionats per fons FEDER, etc.) ha de demanar a l'analista quins indicadors estan associats. L'analista, al seu torn, ha de preguntar a l'enginyer, que sap on estan emmagatzemats, perquè generi alguna cosa (un fitxer, unes taules, etc.) amb tota aquesta informació. Un cop generada la informació, l'analista l'analitza i proporciona una resposta. Aquest cicle es pot repetir fins que l'usuari estigui satisfet amb la proposta. A més, és un procés força estàtic, ja que depèn de l'actualització que hi hagi darrere de les dades.<br/>\n",
    "![Primer cas](datalakes/cerocaso.png  \"Primer cas\")\n",
    "\n",
    "\n",
    "- Quin paper heu jugat en aquest flux?\n",
    "\n",
    "#### Segon escenari\n",
    "\n",
    "Amb l'avenç tecnològic i l'adquisició de certes competències, molts usuaris ja són capaços d'accedir a fonts de dades determinades (serveis de portals de dades obertes, certs repositoris, els seus propis registres, etc.), on alguns departaments publiquen dades prèviament sol·licitades. Aquests usuaris poden processar, analitzar i interpretar els seus propis KPIs. Fantàstic! Tanmateix, no sempre coneixen les particularitats d'algunes dades (captació, actualització, tipologia, semàntica, etc.), cosa que pot conduir a anàlisis incoherents amb les dades d'entrada.<br/>\n",
    "![Segon cas](datalakes/primercaso.png  \"Segon cas\")\n",
    "\n",
    "- Amb aquests cursos esperem que arribeu a realitzar les vostres pròpies anàlisis sense dependre de tercers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El cas desitjable\n",
    "\n",
    "Cal ser sincers i reconèixer les competències professionals de cadascun de nosaltres: és l'analista de dades (o els analistes) qui ha de modelar les dades perquè siguin ofertes coherentment als usuaris de l'organització que les vulguin explotar. Permeteu-nos la metàfora: les dades en brut són meravelloses, però necessiten una mica de cocció per part d'un analista. Processos com la transformació de la periodicitat, el tractament de valors desconeguts o la normalització haurien de ser realitzats pel coneixedor de les dades, l'analista. El bolcatge d'aquestes dades en un Lake o Warehouse proporciona un mecanisme d'accés i càrrega per als altres usuaris.\n",
    "\n",
    "Fixeu-vos que PowerBI té una entrada de dades anomenada Lake.\n",
    "\n",
    "![Tercer cas](datalakes/segundocaso.png  \"Tercer cas\")\n",
    "\n",
    "- Aleshores, com accedeixo a un datalake?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Accés\n",
    "\n",
    "Presentem dos escenaris per accedir a un lake (o warehouse) des de Python i Power BI.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Des de PowerBI\n",
    "\n",
    "Quan creeu un nou informe o afegiu un nou origen de dades, teniu l'opció de seleccionar altres fonts i us apareixerà una finestra com la següent.\n",
    "\n",
    "Entre totes les opcions, trobareu algunes relacionades amb \"Lakes\".\n",
    "\n",
    "![Load lake](datalakes/powerbi-lakes.png \"Load lake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniu en compte que PowerBi és una eina de Microsoft, per tant, té una millor connexió amb les seves eines.\n",
    "Per exemple, amb [OneLake](https://learn.microsoft.com/es-es/fabric/onelake/onelake-overview):<br/>\n",
    "\n",
    "![One lake](datalakes/powerbi-onelake.png  \"One lake\")\n",
    "\n",
    "Si teniu configurat aquest servei directament us surtira el contigut que teniu alla penjat:\n",
    "\n",
    "![One lake contingut](datalakes/powerbi-onelake2.png  \"One lake contingut\")<br/>\n",
    "Només teniu que agafar la font de dades i ja esta! Ràpid i senzill\n",
    "\n",
    "També està molt be integrat amb Azure. I només cal ficar la URL:<br/>\n",
    "![Azure lake](datalakes/powerbi-azurelake.png  \"Azure lake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Des de Python\n",
    "\n",
    "Nota: Penseu que està pensat per a sistemes distribuïts, amb serveis d'autenticació i llibreries més específiques que gestionen informació i distribució de càrregues de treball.\n",
    "\n",
    "Per accedir a un fitxer al nostre sistema, especifiquem la seva ruta (path), per exemple:\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(\"data_samples/WHO.csv\")\n",
    "df = pd.read_csv(\"c:/Usuaris/Isaac/data_samples/WHO.csv\")\n",
    "```\n",
    "\n",
    "Disposem d'un sol sistema operatiu, un disc i, per tant, és senzill especificar la ruta.\n",
    "\n",
    "També podem especificar-ho com una URL:\n",
    "```python\n",
    "df = pd.read_csv(\"http://exploredata.org/ftp/WHO.csv\")\n",
    "```\n",
    "\n",
    "En entorns Lake, sol ser una mica més complex, ja que aquest fitxer estarà distribuït entre diferents equips (per qüestions de rèpliques), i caldrà especificar credencials de seguretat, etc. Adjuntam un exemple amb Python per accedir a un [Lake a Google](https://cloud.google.com/python/docs/reference/dataplex/latest).\n",
    "\n",
    "```python\n",
    "from google.cloud import storage\n",
    "from google.cloud import dataplex_v1\n",
    "import os\n",
    "# initialize the common variables\n",
    "dataplex_dict= {\n",
    "    \"project\": \"{PROJECT_ID}\", \n",
    "    \"region\": \"none\",\n",
    "    \"gcs_bucket_name\":\"{SOURCE_BUCKET}\",\n",
    "    \"zone_type\": \"RAW\",\n",
    "    \"zone_location_type\": \"SINGLE_REGION\",\n",
    "    \"zone_id\": \"{ZONE_ID}\",\n",
    "    \"asset_type\":\"STORAGE_BUCKET\",\n",
    "    \"asset_id\":\"{ASSET_ID}\",\n",
    "    \"asset_name\":\"projects/{PROJECT_ID}/buckets/{SOURCE_BUCKET}\",\n",
    "    \"bq_dataset\":\"none\"\n",
    "}\n",
    "# authenticate using service account key\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'key.json'\n",
    "# obtain the bucket that we will attach to the zone\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(dataplex_dict[\"gcs_bucket_name\"])\n",
    "# update the dict's location. This will be used to create a lake if doesn't exists\n",
    "dataplex_dict[\"region\"] = bucket.location.lower()\n",
    "# Create a dataplex client\n",
    "dataplex_client = dataplex_v1.DataplexServiceClient()\n",
    "```\n",
    "\n",
    "Quan tenim el client, podem llegir informació o amb el propi bucket\n",
    "\n",
    "```python\n",
    "...\n",
    "import pandas as pd\n",
    "import gcsfs\n",
    "\n",
    "fs = gcsfs.GCSFileSystem(project=\"{PROJECT_ID}\")\n",
    "with fs.open('\"{SOURCE_BUCKET}\"/path.csv') as f:\n",
    "    df = pd.read_csv(f)\n",
    "```    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "- **Els datalakes i els data warehouses** són conceptes relacionats amb nous serveis dissenyats per facilitar, consolidar i emmagatzemar dades de manera centralitzada per a l'accés del personal d'una organització.\n",
    "- **Requereixen un pla de governança** i la implicació dels principals actors, com ara analistes de dades i enginyers de sistemes i de bases de dades.\n",
    "- **Necessiten una infraestructura** que ofereixi aquest servei d'ingesta i els serveis operacionals (còpies de seguretat, xifrat, autenticació, etc.).\n",
    "- **El consum d'informació mitjançant un llenguatge de programació** no és senzill i no està dissenyat per a entorns interactius com Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi ha un parell de lllibres que són molt interesants. Cobreixen aquest paradigme des de diferents perfills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referències:\n",
    "\n",
    "- 1. Designing Data-Intensive Appliations. Martin Kleppmann. OReally 2017<br/>\n",
    "- 2. The Enterprise Big Data Lake: Delivering the Promise of Big Data and Data Science. Alex Gorelik. O'Really. 2019<br/>\n",
    "- 3. Data Lake for Enterprises. Tomcy John, Pankaj Misra. Packt. 2017<br/>\n",
    "- 4. The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling 3rd Edition. Ralph Kimball, Margy Ross. Wiley. 2013<br/>\n",
    "- 5. The Analytics Setup Guidebook How to build scalable analytics & BI stacks in the modern cloud era. Huy Nguyen, Ha Pham, Cedric Chin. Holistics, 2020<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![License: CC BY 4.0](https://img.shields.io/badge/License-CC_BY_4.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/) <br/>\n",
    "Authors: [Isaac Lera](https://personal.uib.cat/isaac.lera), [Miquel Miró](https://personal.uib.cat/miquel.miro) and [Biel Moyà](https://personal.uib.cat/gabriel.moya)<br/>\n",
    "Institution: Universitat de les Illes Balears (UIB) <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
