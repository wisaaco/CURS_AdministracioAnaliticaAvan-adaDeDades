{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unitat 9. Introducció a data lakes i data warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquests dos conceptes fan referència a arquitectures on s'emmagatzemen dades amb l'objectiu de facilitar la integració de dades de diferents serveis d'una organització a llarg plaç. Aquest tipus d'arquitectures impliquem que al darrera hi ha un projecte de governanza de les dades. La presència d'aquestes entitats s'acompanya d'una guia de bones pràctiques per a la recopilació, integració i organització de dades que ha de ser entesa per els departaments d'una organització.<br/>\n",
    "\n",
    "Naturalment, està format per clusters d'equip de diferent naturaleza i composició, a més inclou un grup heterogeni de programari que centralitcen les dades des d'una perspectiva d'explotació oferim punts d'entrada per als usuaris on puguin consultar  dades de intereses.\n",
    "\n",
    "En aquestes arquitectures hi ha processos operacionals com còpies de seguretat, xifrat i encriptació, autenticació i accés, eines per a l'execució distribuïda de dades, etc. \n",
    "\n",
    "Atès que són conceptes propis de l'enginyeria i perfils molt especialitzats com el d'arquitecte de cloud o de dades, **en aquest capítol ens centrarem  a adquirir nocions bàsiques de tots dos conceptes i entendre els avantatges que aquests sistemes aporten a una organització i veurem con funciona des de el punt de vista d'un usuari**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Nocions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando una organización se subdivide en departamentos, grupos y proyectos cada uno genera y depende de unos datos especificos. Una visión holísitca de los datos suele ser complicada sino hay una clara gorvenanza de los datos. En ella se define que registrar, como registrarlos, durante cuanto tiempo, y otro de las cuestiones importantes es dónde, entre otros factores.\n",
    "\n",
    "##### Ese dónde es un data lake y/o un warehouse.\n",
    "\n",
    "#### La metafora \n",
    "Un lago (*lake*) o un deposito (warehouse) no es más que una metafora con el ciclo del agua y el flujo de datos. Los datos son como el agua, muchos circulan dentro de la organización sin que sean necesarios guardarlos. Sin embargo, en ciertas situaciones es conveniente registar y mantener unos cuantos, por ello se forma un lago donde queda esa acumulación de datos/agua. \n",
    "\n",
    "- En un lago (lake) el agua es almacenada en crudo, es decir, no se realizan procesos de limpieza o de tratamiento. \n",
    "\n",
    "- En un deposito o tanque (warehouse), el agua es tratada, es decir, se realizan procesos ETL (extracción, transformación, carga).\n",
    "\n",
    "Según el contexto ambos términos acaban confundiendose. Hoy en día, ciertas herramientas pensadas para dar soporte a funciones Lake ofrecen mecanismos para hacer transformaciones.\n",
    "\n",
    "![ETL](datalakes/ETL.jpg \"ETL\")\n",
    "\n",
    "\n",
    "\n",
    "A nivel práctico, los grandes proveedores de Cloud como Google, AWS (Amazon), Azure (Microsfot), Alibaba entre otros ofrecen servicios específicos que facilitan la integración y una perspectiva centralizada de los datos. La centralización es con respecto al conocimiento de su existencia, sus metadatos, y su ofrecimiento a otros usuarios de la organización.\n",
    "\n",
    "En la figura vemos el panel de creación de Lakes dentro de Google Cloud (este servicio en su entorno es llamado Dataplex), donde se aprecia funciones de administración, seguridad, procesos operacionales, gestión de atributos, perfiles de verificación de los datos, o evaluación de los mismos. \n",
    "![Google Lake](datalakes/googlelake.png  \"Google Lake\")\n",
    "\n",
    "En AWS también encontramos el mismo servicio, como AWS Lake Foundation donde se integran procesos ETL\n",
    "![AWS Lake](datalakes/AWS.webp  \"AWS Lake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen **alternativas abiertas y gratuitas** para crear Lagos. La más conocida es [Apache Hudi](https://hudi.apache.org/), dentro del ecosistema de [Apache](https://www.apache.org/). Una fundación que acoge la mayor parte de proyectos en abierto y relacionados con la web, entre ellos las principales bases de datos y modelos de computación como Apache Spark (similar al implementado por Google) que ofrece analitica en grandes volumnes de datos.\n",
    "\n",
    "![Apache Hudi](datalakes/ApacheHudi.png  \"Apache Hudi\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esperamos que se entienda tras estos breves comentarios y figuras que la configuración, y la puesta en marcha de un data lake requiere perfiles especialidos en Administración de Sistemas, Cloud Arquicture or Data Architecturezs.\n",
    "\n",
    "Además están pensados para el procesamiento de grandes cantidades de datos en muy poco tiempo por ello se necesitan librerías especializadas de computación paralela o distribuida. Es decir, no suele ser habitual usar Jupyter Notebooks o similares como Google Colab para ejecutar scripts de Pandas en un data lake. Necesitamos recurrir a librerías como Spark y paradigmas de programación de divide y vencerás (MapReduce)\n",
    "Para que nos hagamos una idea, imaginad que tenemos que traducir toda una biblioteca de documentos. Una sola persona tardará bastante tiempo pero si disponemos de 100 personas tardaremos casi 100 veces menos (no es proporcional la ganancia ya que hay que considerar el tiempo en organizar a esas personas y repartir el trabajo). En ese sistema, cada persona a su vez podrá distribuir tareas de traducción sobre cada capítulo de libro. Imaginad lo poco que se tarda si somos capaces de dividir el problema de esa manera. Eso sí, necesitamos una infraestructura que sea capaz de repartir trabajos entre trabajadores, coordinarlos y coordinar los resultados de todos ellos. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aportacions: I a mi que m'aporta un data lake?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que os queremos transmitir y enseñar con esta unidad??\n",
    "\n",
    "Tened en cuenta que aparte de la infraestructura, detras de un data lake hay una evoluvión en la organización de los datos. Una politica de transparencia y coherencia donde todos los perfiles de una organización se beneficián de ello.\n",
    "\n",
    "Os ponemos dos escenarios! seguro que os habéis encontrado en esa situación alguna vez...\n",
    "\n",
    "Nota: Fuente de imágenes de: Referències [5] The Analytics Setup Guidebook How to build scalable analytics & BI stacks in the modern cloud era. Huy Nguyen, Ha Pham, Cedric Chin. Holistics, 2020<br/>\n",
    "\n",
    "\n",
    "#### Primer escenario\n",
    "Un usuario que quiere conocer un indicador (KPI) de la organización (por ejemplo, ventas totales, solicitantes de una ayuda, proyectos subvencionados por fondes feder, etc.) ha de solicitar al analista que indicadores están asociados y esté a su vez ha de preguntar al ingenerio que conoce donde están almacenados que le generé algo (un fichero, unas tablas, etc.) con toda esa información, una vez generada la información el analista la analiza y proporciona una respuesta. Este ciclo se puede repetir hasta que el usuario esté satisfecho con la propuesta y a su vez es un proceso bastante estático ya que depende de la actualización que hay por detrás de los datos.\n",
    "![Primer caso](datalakes/cerocaso.png  \"Primer caso\")\n",
    "\n",
    "- Qué papel habéis desempeñado en ese flujo?\n",
    "\n",
    "#### Segundo escenario\n",
    "Con el avance tecnológico y la adquisición de ciertas competencias, mucho susuarios ya son capaces de acceder a determinadas fuentes de datos (servicios de portales de datos abiertos, ciertos repositorios abiertos, etc.) donde algunos departamentos publican datos previamente solicitados, puede procesar datos, analizarlos e interpretar sus KPIs. Es fantástico! Sin embargo, los usuarios no son conocedores de la idiosincracia de algunos datos (captación, actualización, tipología, semántica, etc.) dando lugar a análisis incoherentes con la entrada.\n",
    "![Segundo caso](datalakes/primercaso.png  \"Segundo caso\")\n",
    "\n",
    "\n",
    "- Con estos cursos esperamos que lleguéis a realizar vuestros proprios analisis y no dependais de terceros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El caso deseable\n",
    "\n",
    "Hay que ser sincero y reconocer las competencias profesionales de cada uno de nosotros, es el analista de datos (o los analistas) los que han de modelar los datos para que estos sean ofrecidos coherentemente a los usuarios de la organización que quieran explotarlos. Permitidnos la metafora, los datos en crudo son maravillosos pero han de cocinarse un poco por un analista. Procesos como la transformación de la periocidad, el tratanmiento de valores desconocidos, o la normalización deberían de realizarse por el conocedor de los datos, el analista. El volcado de estos datos en un Lake o Warehouse proporciona un mecanismo de carga al resto de usuarios.<br/>\n",
    "\n",
    "Fijaros que PowerBI tiene una entrada de datos llamada Lake. \n",
    "\n",
    "![Tercer caso](datalakes/segundocaso.png  \"Tercer caso\")\n",
    "\n",
    "- Entonces, ¿cómo accedo a un datalake?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Acceso\n",
    "\n",
    "Presentamos dos escenarios para acceder a un lake (o warehouse) desde Python y  PowerBi.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desde PowerBi\n",
    "\n",
    "Comenzamos con la opción fàcil. Para acceder a un fichero en nuestro sistema, especificamos su ruta (path), por ejemplo:\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(\"data_samples/WHO.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desde Python\n",
    "\n",
    "Nota: Pensad que está pensado para sistemas distribuidos, con servicios de autentificación, y librerías más especificas que gestiona información y distribución de cargas de trabajo.\n",
    "\n",
    "Para acceder a un fichero en nuestro sistema, especificamos su ruta (path), por ejemplo:\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(\"data_samples/WHO.csv\")\n",
    "df = pd.read_csv(\"c:/Usuarios/Isaac/data_samples/WHO.csv\")\n",
    "```\n",
    "\n",
    "Disponemos de un solo Sistema Operativo, un disco y por tanto, es sencillo especificar la ruta.\n",
    "\n",
    "\n",
    "También podemos especificarlo como una URL:\n",
    "```python\n",
    "df = pd.read_csv(\"http://exploredata.org/ftp/WHO.csv\")\n",
    "```\n",
    "\n",
    "\n",
    "En entornos Lake, suele ser un poco más complejo, ya que ese fichero estará distribuido entre diferentes equipos (por cuestiones de replicas), habrá que especificar credenciales de seguridad, etc.\n",
    "Adjuntamos un ejemplo con Python para acceder a un Lake en Google [https://cloud.google.com/python/docs/reference/dataplex/latest].\n",
    "\n",
    "```python\n",
    "from google.cloud import storage\n",
    "from google.cloud import dataplex_v1\n",
    "import os\n",
    "# initialize the common variables\n",
    "dataplex_dict= {\n",
    "    \"project\": \"{PROJECT_ID}\", \n",
    "    \"region\": \"none\",\n",
    "    \"gcs_bucket_name\":\"{SOURCE_BUCKET}\",\n",
    "    \"zone_type\": \"RAW\",\n",
    "    \"zone_location_type\": \"SINGLE_REGION\",\n",
    "    \"zone_id\": \"{ZONE_ID}\",\n",
    "    \"asset_type\":\"STORAGE_BUCKET\",\n",
    "    \"asset_id\":\"{ASSET_ID}\",\n",
    "    \"asset_name\":\"projects/{PROJECT_ID}/buckets/{SOURCE_BUCKET}\",\n",
    "    \"bq_dataset\":\"none\"\n",
    "}\n",
    "# authenticate using service account key\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'key.json'\n",
    "# obtain the bucket that we will attach to the zone\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(dataplex_dict[\"gcs_bucket_name\"])\n",
    "# update the dict's location. This will be used to create a lake if doesn't exists\n",
    "dataplex_dict[\"region\"] = bucket.location.lower()\n",
    "# Create a dataplex client\n",
    "dataplex_client = dataplex_v1.DataplexServiceClient()\n",
    "```\n",
    "\n",
    "Cuando tenemos el cliente, podemos leer información o con el propio bucket.\n",
    "\n",
    "```python\n",
    "...\n",
    "import pandas as pd\n",
    "import gcsfs\n",
    "\n",
    "fs = gcsfs.GCSFileSystem(project=\"{PROJECT_ID}\")\n",
    "with fs.open('\"{SOURCE_BUCKET}\"/path.csv') as f:\n",
    "    df = pd.read_csv(f)\n",
    "```    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referències:\n",
    "\n",
    "- 1. Designing Data-Intensive Appliations. Martin Kleppmann. OReally 2017<br/>\n",
    "- 2. The Enterprise Big Data Lake: Delivering the Promise of Big Data and Data Science. Alex Gorelik. O'Really. 2019<br/>\n",
    "- 3. Data Lake for Enterprises. Tomcy John, Pankaj Misra. Packt. 2017<br/>\n",
    "- 4. The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling 3rd Edition. Ralph Kimball, Margy Ross. Wiley. 2013<br/>\n",
    "- 5. The Analytics Setup Guidebook How to build scalable analytics & BI stacks in the modern cloud era. Huy Nguyen, Ha Pham, Cedric Chin. Holistics, 2020<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![License: CC BY 4.0](https://img.shields.io/badge/License-CC_BY_4.0-lightgrey.svg)](https://creativecommons.org/licenses/by/4.0/) <br/>\n",
    "Authors: [Isaac Lera](https://personal.uib.cat/isaac.lera), [Miquel Miró](https://personal.uib.cat/miquel.miro) and [Biel Moyà](https://personal.uib.cat/gabriel.moya)<br/>\n",
    "Institution: Universitat de les Illes Balears (UIB) <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
